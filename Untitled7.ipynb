{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f475806-71f7-4c74-90d5-df1d19f3b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "# algorithms that are not affected by missing values.\n",
    "# Ans 1:Missing values in a dataset are the values that are not present for a particular variable or observation. \n",
    "# These values can be represented by various symbols, such as \"NA\", \"NaN\", or \"?\".\n",
    "# Missing Values\n",
    "# Missing values occurs in dataset when some of the informations is not stored for a variable There are 3 mechanisms:\n",
    "# 1.Missing Completely at Random, MCAR\n",
    "# 2.Missing at Random MAR\n",
    "# 3.Missing data not at random MNAR\n",
    "# Some algorithms that are not affected by missing values include tree-based models, such as decision trees,\n",
    "# random forests, and gradient boosting machines, as they can automatically handle missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc28e3e-4475-4a1e-a90e-9041419c0a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "# Ans 2 :here are several techniques used to handle missing data in a dataset. Here are some common techniques with examples in Python:\n",
    "\n",
    "# Deletion: This technique involves removing the rows or columns containing missing values. There are three types of deletion:\n",
    "# a. Listwise Deletion: This method involves deleting the entire row that contains the missing value.\n",
    "import seaborn as sns \n",
    "df=sns.load_dataset('titanic')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea64277-6da5-41cf-9404-fab171611b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## rowwise deletion\n",
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f31ec4-59bb-4891-8f33-811f9e8cae88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex  sibsp  parch     fare   class    who  \\\n",
       "0           0       3    male      1      0   7.2500   Third    man   \n",
       "1           1       1  female      1      0  71.2833   First  woman   \n",
       "2           1       3  female      0      0   7.9250   Third  woman   \n",
       "3           1       1  female      1      0  53.1000   First  woman   \n",
       "4           0       3    male      0      0   8.0500   Third    man   \n",
       "..        ...     ...     ...    ...    ...      ...     ...    ...   \n",
       "886         0       2    male      0      0  13.0000  Second    man   \n",
       "887         1       1  female      0      0  30.0000   First  woman   \n",
       "888         0       3  female      1      2  23.4500   Third  woman   \n",
       "889         1       1    male      0      0  30.0000   First    man   \n",
       "890         0       3    male      0      0   7.7500   Third    man   \n",
       "\n",
       "     adult_male alive  alone  \n",
       "0          True    no  False  \n",
       "1         False   yes  False  \n",
       "2         False   yes   True  \n",
       "3         False   yes  False  \n",
       "4          True    no   True  \n",
       "..          ...   ...    ...  \n",
       "886        True    no   True  \n",
       "887       False   yes   True  \n",
       "888       False    no  False  \n",
       "889        True   yes   True  \n",
       "890        True    no   True  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59263453-c71c-465c-bdec-1f63d006da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_mean']=df['age'].fillna(df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45443fa8-4c84-4549-96ae-d3456736c850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "      <th>Age_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone   Age_mean  \n",
       "0      man        True  NaN  Southampton    no  False  22.000000  \n",
       "1    woman       False    C    Cherbourg   yes  False  38.000000  \n",
       "2    woman       False  NaN  Southampton   yes   True  26.000000  \n",
       "3    woman       False    C  Southampton   yes  False  35.000000  \n",
       "4      man        True  NaN  Southampton    no   True  35.000000  \n",
       "..     ...         ...  ...          ...   ...    ...        ...  \n",
       "886    man        True  NaN  Southampton    no   True  27.000000  \n",
       "887  woman       False    B  Southampton   yes   True  19.000000  \n",
       "888  woman       False  NaN  Southampton    no  False  29.699118  \n",
       "889    man        True    C    Cherbourg   yes   True  26.000000  \n",
       "890    man        True  NaN   Queenstown    no   True  32.000000  \n",
       "\n",
       "[891 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764e74b-1ee8-42b4-9063-b33d8e10c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "# Ans 3:Imbalanced data refers to a situation in which the distribution of classes in a dataset is not equal. In other words, some\n",
    "# classes have significantly more examples than others. For example, if a binary classification problem has 90% positive examples\n",
    "# and only 10% negative examples, the dataset is imbalanced.\n",
    "\n",
    "# If imbalanced data is not handled, it can lead to biased machine learning models that perform poorly on the minority class. \n",
    "# In particular, the model will tend to predict the majority class more often than the minority class, which can result in very\n",
    "# high accuracy but poor performance in practice. This is because the model may not have learned enough about the minority class \n",
    "# to make accurate predictions, due to the lack of training examples.\n",
    "\n",
    "# For example, in the case of medical diagnosis, if the data is imbalanced and there are more healthy patients than sick ones,\n",
    "# a model trained on such data will tend to predict that a new patient is healthy, even if they have symptoms of the disease. \n",
    "# This can lead to missed diagnoses and serious consequences for the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2bf25-ddf6-4d4c-8311-814d523a79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "# sampling are required.\n",
    "# Ans-\n",
    "#      Up-sampling and down-sampling are techniques used to balance the class distribution in an imbalanced dataset.\n",
    "\n",
    "# Down-sampling involves randomly removing samples from the majority class to match the number of samples in the minority class. This technique is used when the dataset is large, and the majority class has a much higher number of samples than the minority class. For example, in a dataset with 1000 samples, where 950 samples belong to the majority class and 50 samples belong to the minority class, down-sampling can be used to randomly remove 900 samples from the majority class, leaving 50 samples in each class.\n",
    "\n",
    "#     Here's an example of down-sampling using the Python sklearn library:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# assuming we have majority_class_samples and minority_class_samples as dataframes\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(majority_class_samples, replace=False, n_samples=len(minority_class_samples), random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled_df = pd.concat([downsampled_majority, minority_class_samples])\n",
    "# Up-sampling, on the other hand, involves randomly replicating samples from the minority class to increase their number and match the number of samples in the majority class. This technique is used when the dataset is small, and the minority class has much fewer samples than the majority class. For example, in a dataset with 100 samples, where 10 samples belong to the minority class and 90 samples belong to the majority class, up-sampling can be used to randomly replicate the minority class samples to make a total of 90 samples in each class.\n",
    "\n",
    "    Here's an example of up-sampling using the Python sklearn library:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# assuming we have majority_class_samples and minority_class_samples as dataframes\n",
    "# Upsample minority class\n",
    "upsampled_minority = resample(minority_class_samples, replace=True, n_samples=len(majority_class_samples), random_state=42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_df = pd.concat([majority_class_samples, upsampled_minority])\n",
    "# It is important to note that both up-sampling and down-sampling have their own advantages and disadvantages, and the choice between them should be based on the specific problem and dataset at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec1896-3f8c-4c87-82ef-c6e8e898dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE.\n",
    "# Ans-\n",
    "#     Data augmentation is a technique used to increase the size of a dataset by creating new examples by applying various transformations on the existing data. It is a commonly used technique in machine learning to deal with limited data availability and to improve the performance of models.\n",
    "\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation method used to deal with imbalanced datasets. It creates synthetic samples of the minority class by generating new examples based on the existing minority class samples. The basic idea of SMOTE is to interpolate between the feature vectors of the minority class samples to create new synthetic samples.\n",
    "\n",
    "# The SMOTE algorithm works as follows:\n",
    "\n",
    "# 1.For each sample in the minority class, find its k nearest neighbors.\n",
    "# 2.Select one of the k neighbors randomly, and compute the difference between the feature vector of the sample and the feature vector of the selected neighbor.\n",
    "# 3.Multiply this difference by a random value between 0 and 1, and add the result to the feature vector of the minority class sample to create a new synthetic sample.\n",
    "# 4.Repeat steps 2 and 3 to create the desired number of new synthetic samples.\n",
    "\n",
    "# SMOTE helps to balance the class distribution in the dataset and improves the performance of machine learning models, especially for classification problems. It is commonly used in applications such as fraud detection, medical diagnosis, and customer churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adae2a47-1b2b-4dea-a8b0-545cc3f116b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (100, 3)\n",
      "Cleaned data shape: (96, 3)\n",
      "Original data:\n",
      "            A         B         C\n",
      "0  10.000000 -1.311498  1.290997\n",
      "1  -0.748009  0.208490 -0.418629\n",
      "2  -0.465340 -0.400057  0.219025\n",
      "3  -0.317286  0.353893 -1.733128\n",
      "4   0.497959  0.389372 -0.474089\n",
      "\n",
      "Winsorized data:\n",
      "           A         B         C\n",
      "0  1.691657 -1.247918  1.290997\n",
      "1 -0.748009  0.208490 -0.418629\n",
      "2 -0.465340 -0.400057  0.219025\n",
      "3 -0.317286  0.353893 -1.524117\n",
      "4  0.497959  0.389372 -0.474089\n"
     ]
    }
   ],
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "# Ans-\n",
    "#    Outliers are data points that lie far away from the rest of the data in a dataset. Outliers can occur due to various reasons such as measurement errors, incorrect data entry, or natural variation in the data.\n",
    "\n",
    "# It is essential to handle outliers because they can have a significant impact on statistical analyses and machine learning models. Outliers can lead to inaccurate estimates of summary statistics, biased results, and reduced model performance.\n",
    "\n",
    "# There are several techniques to handle outliers, including:\n",
    "\n",
    "# 1.Removing outliers: In this approach, outliers are identified and removed from the dataset. However, this approach can lead to a loss of information and may not always be the best option.\n",
    "\n",
    "# 2.Winsorizing: This approach involves capping the extreme values in the dataset at a certain percentile value. This method can help reduce the influence of outliers without completely removing them.\n",
    "\n",
    "# 3.Transformation: This approach involves transforming the data to reduce the impact of outliers. For example, taking the log of the data can help normalize the distribution and reduce the impact of outliers.\n",
    "\n",
    "# 4.Robust statistical methods: These methods are designed to be less sensitive to outliers. For example, the median is a more robust measure of central tendency than the mean.\n",
    "\n",
    "# 5.Machine learning algorithms: Some machine learning algorithms, such as decision trees and random forests, are less sensitive to outliers than others.\n",
    "\n",
    "#     Example of outlier removal with Python code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataset with outliers\n",
    "data = pd.DataFrame({'A': np.random.normal(0, 1, 100),\n",
    "                     'B': np.random.normal(0, 1, 100),\n",
    "                     'C': np.random.normal(0, 1, 100)})\n",
    "data.loc[0, 'A'] = 10  # add outlier\n",
    "\n",
    "# identify and remove outliers\n",
    "q1 = data.quantile(0.25)\n",
    "q3 = data.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "data_clean = data[~((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))).any(axis=1)]\n",
    "\n",
    "print('Original data shape:', data.shape)\n",
    "print('Cleaned data shape:', data_clean.shape)\n",
    "\n",
    "   # Example of Winsorizing with Python code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataset with outliers\n",
    "data = pd.DataFrame({'A': np.random.normal(0, 1, 100),\n",
    "                     'B': np.random.normal(0, 1, 100),\n",
    "                     'C': np.random.normal(0, 1, 100)})\n",
    "data.loc[0, 'A'] = 10  # add outlier\n",
    "\n",
    "# Winsorize the extreme values\n",
    "p = 0.05  # percentiles to cap at\n",
    "data_winsorized = data.apply(lambda x: np.clip(x, x.quantile(p), x.quantile(1-p)))\n",
    "\n",
    "print('Original data:\\n', data.head())\n",
    "print('\\nWinsorized data:\\n', data_winsorized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c037098-f1c3-4c73-99ab-e8dd77be8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "# the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "# Ans-\n",
    "#    There are several techniques that can be used to handle missing data in customer data analysis. Some of these techniques are:\n",
    "\n",
    "# 1.Deletion: This technique involves removing the rows or columns that contain missing values. This technique can be used when the missing values are very few in number and are randomly distributed. There are two types of deletion techniques:\n",
    "\n",
    "#     a. Listwise deletion: In this technique, the entire row is deleted if it contains any missing value.\n",
    "\n",
    "#     b. Pairwise deletion: In this technique, only the missing values are ignored for each calculation.\n",
    "\n",
    "# 2.Mean/median imputation: In this technique, the missing values are replaced with the mean or median value of the respective feature. This technique can be used when the missing values are few and the data is normally distributed.\n",
    "\n",
    "# 3.Mode imputation: In this technique, the missing values are replaced with the mode value of the respective feature. This technique can be used when the missing values are few and the data is categorical.\n",
    "\n",
    "# 4.Regression imputation: In this technique, a regression model is used to predict the missing values based on the values of other features. This technique can be used when the missing values are non-random and the data has a linear relationship.\n",
    "\n",
    "# Example code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating a sample dataset with missing values\n",
    "data = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_mean = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Mode imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_mode = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Regression imputation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "x_train = data.dropna().drop('A', axis=1)\n",
    "y_train = data.dropna()['A']\n",
    "x_test = data[data['A'].isna()].drop('A', axis=1)\n",
    "y_test = lin_reg.fit(x_train, y_train).predict(x_test)\n",
    "data_regression = data.copy()\n",
    "data_regression.loc[data['A'].isna(), 'A'] = y_test\n",
    "\n",
    "# Deletion\n",
    "data_deletion = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e17fa-56b8-4a2f-8cf5-a8b65018497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "Ans-\n",
    "   There are various strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some common techniques:\n",
    "\n",
    "1.Visualization: One approach is to create visualizations of the missing data. For example, you can use a heatmap to visualize the missing data across different variables. If the missing data is random, then the heatmap will show a random pattern of missing data across different variables. However, if there is a pattern to the missing data, then the heatmap will show clusters of missing data across certain variables.\n",
    "\n",
    "2.Statistical tests: Another approach is to use statistical tests to determine if the missing data is missing at random or not. One common test is the Little's MCAR (Missing Completely At Random) test. This test checks if the missing data is independent of both observed and unobserved data. If the test fails to reject the null hypothesis, then the missing data can be considered missing at random.\n",
    "\n",
    "3.Imputation: Another approach is to use imputation methods to fill in the missing data. By comparing the imputed values to the actual values, it is possible to determine if there is a pattern to the missing data. For example, if the imputed values are consistently higher or lower than the actual values, then there may be a systematic bias in the missing data.\n",
    "\n",
    "Overall, it is important to carefully examine the missing data to determine if it is missing at random or if there is a pattern to the missing data. This can help inform the choice of imputation method or other strategies for handling the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1435d-1520-463b-9fd8-4df23b9ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "Ans-\n",
    "    When working with imbalanced datasets, it's important to use evaluation metrics that are sensitive to both the minority and majority classes. Some strategies for evaluating the performance of a machine learning model on an imbalanced dataset include:\n",
    "\n",
    "1.Confusion matrix: A confusion matrix provides information on the true positives, true negatives, false positives, and false negatives in a classification problem. It's a useful tool for understanding the performance of a model on imbalanced datasets.\n",
    "\n",
    "2.Precision, Recall, and F1-score: Precision is the ratio of true positives to the total number of positive predictions made by the model. Recall is the ratio of true positives to the total number of actual positive cases in the dataset. F1-score is the harmonic mean of precision and recall. These metrics are useful for evaluating the performance of a model on imbalanced datasets.\n",
    "\n",
    "3.ROC-AUC Curve: ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) curve is another useful tool for evaluating the performance of a model on imbalanced datasets. It shows the trade-off between sensitivity (recall) and specificity (true negative rate) at different classification thresholds.\n",
    "\n",
    "4.Resampling Techniques: There are several resampling techniques that can be used to balance the class distribution in the dataset. For example, upsampling the minority class, downsampling the majority class, or using synthetic data generation techniques like SMOTE. These techniques can be used to balance the dataset before training the model.\n",
    "\n",
    "5.Cost-Sensitive Learning: Cost-sensitive learning is a technique that assigns different costs to different types of errors. For example, a false negative in a medical diagnosis task can be more costly than a false positive. By assigning different costs to different types of errors, the model can be optimized to minimize the overall cost of errors.\n",
    "\n",
    "6.Ensemble Learning: Ensemble methods can be used to combine multiple models to improve the overall performance on imbalanced datasets. For example, a combination of oversampled and undersampled models can be used to achieve better performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f01d74-f155-4463-abc7-d5145337137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "Ans-\n",
    "    If the majority class is overrepresented in a dataset, it can be downsampled to balance the dataset. Here are some methods that can be employed to balance the dataset and down-sample the majority class:\n",
    "\n",
    "1.Random under-sampling: In this method, a random sample of the majority class is removed to balance the dataset. However, this method may result in the loss of useful information.\n",
    "\n",
    "   Here's an example of random under-sampling using Python:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction == 'satisfied']\n",
    "minority_class = df[df.satisfaction == 'unsatisfied']\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "# Check the class distribution\n",
    "downsampled.satisfaction.value_counts()\n",
    "\n",
    "2.Cluster-based under-sampling: In this method, the majority class is divided into clusters, and a representative sample is taken from each cluster.\n",
    "\n",
    "3.Tomek links: In this method, the samples of the majority and minority class are linked, and the samples that form a Tomek link are identified. The majority class samples that form Tomek links are removed.\n",
    "\n",
    "4.Edited nearest neighbors: In this method, the majority class samples that are misclassified by their k-nearest neighbors are removed.\n",
    "\n",
    "5.Synthetic Minority Over-sampling Technique (SMOTE): This method creates synthetic samples of the minority class by interpolating between the existing samples.\n",
    "\n",
    "   Here's an example of using SMOTE for up-sampling the minority class in Python:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction == 'satisfied']\n",
    "minority_class = df[df.satisfaction == 'unsatisfied']\n",
    "\n",
    "# Upsample minority class using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# Check the class distribution\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57af2ea-11fd-4163-8f83-ed70e1e2e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "Ans-\n",
    "   If the dataset is unbalanced with a low percentage of occurrences of the event of interest, we can use the following methods to balance the dataset and up-sample the minority class:\n",
    "\n",
    "1.Random oversampling: This method involves randomly selecting samples from the minority class with replacement to create a balanced dataset.\n",
    "\n",
    "2.Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a popular technique for oversampling the minority class. It involves creating synthetic examples of the minority class by interpolating between neighboring samples.\n",
    "\n",
    "3.ADASYN: ADASYN is another technique that is similar to SMOTE but focuses on generating synthetic samples in regions where the class imbalance is highest.\n",
    "\n",
    "    Here is an example of how to use the SMOTE technique in Python:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n",
    "\n",
    "In this code snippet, X and y are the feature and target variables, respectively. The SMOTE() function is used to perform the oversampling operation, and the fit_resample() method is used to transform the data. The resulting X_resampled and y_resampled arrays contain the balanced dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
